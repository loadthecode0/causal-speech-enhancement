{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bvZ7vSCOKJV",
        "outputId": "bed7c299-5eaf-4a27-a496-004f3fe3e62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'causal-speech-enhancement'...\n",
            "remote: Enumerating objects: 748, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 748 (delta 122), reused 84 (delta 80), pack-reused 595 (from 3)\u001b[K\n",
            "Receiving objects: 100% (748/748), 1.38 GiB | 23.73 MiB/s, done.\n",
            "Resolving deltas: 100% (445/445), done.\n",
            "Updating files: 100% (45/45), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/loadthecode0/causal-speech-enhancement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('causal-speech-enhancement')"
      ],
      "metadata": {
        "id": "vrtTU4-BOaLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing data\n"
      ],
      "metadata": {
        "id": "KqubBmA-N3_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up dataset\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EARSWHAMAudioDataset(Dataset):\n",
        "    def __init__(self, base_dir = \"../datasets_final/EARS-WHAM16kHz\", dataset=\"train\", transform=None, seg_length = 16000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_dir (str): Path to the base directory containing train, valid, and test subdirectories.\n",
        "            dataset (str): Dataset split to use (\"train\", \"valid\", \"test\").\n",
        "            transform (callable, optional): Optional transform to apply to the audio data.\n",
        "        \"\"\"\n",
        "        assert dataset in [\"train\", \"valid\", \"test\"], \"Invalid dataset split. Choose from 'train', 'valid', or 'test'.\"\n",
        "        self.transform = transform\n",
        "        self.seg_length = seg_length\n",
        "\n",
        "         # Set directories for clean and noisy files based on the dataset split\n",
        "        self.clean_dir = os.path.join(base_dir, dataset, \"clean\")\n",
        "        self.noisy_dir = os.path.join(base_dir, dataset, \"noisy\")\n",
        "        # Gather and sort file names to ensure pairing\n",
        "        self.clean_files = sorted(glob.glob(os.path.join(self.clean_dir, \"**/*.wav\"), recursive=True))\n",
        "        self.noisy_files = sorted(glob.glob(os.path.join(self.noisy_dir, \"**/*.wav\"), recursive=True))\n",
        "\n",
        "        # Ensure clean and noisy file lists match\n",
        "        assert len(self.clean_files) == len(self.noisy_files), \\\n",
        "            f\"Mismatch in the number of clean and noisy files for {dataset} dataset.\"\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_waveform, _ = torchaudio.load(self.clean_files[idx])\n",
        "        noisy_waveform, _ = torchaudio.load(self.noisy_files[idx])\n",
        "\n",
        "        audio_length = clean_waveform.size(1)\n",
        "\n",
        "        #We find a random starting point for the segment\n",
        "        if audio_length > self.seg_length:\n",
        "            start = torch.randint(0, audio_length - self.seg_length, (1, )).item()\n",
        "            clean_waveform = clean_waveform[:, start: start + self.seg_length]\n",
        "            noisy_waveform = noisy_waveform[:, start:start + self.seg_length]\n",
        "        else:\n",
        "            pad_length = self.seg_length - audio_length\n",
        "            clean_waveform = torch.nn.functional.pad(clean_waveform, (0, pad_length))\n",
        "            noisy_waveform = torch.nn.functional.pad(noisy_waveform, (0, pad_length))\n",
        "\n",
        "\n",
        "        # Apply optional transformation\n",
        "        if self.transform:\n",
        "            clean_waveform = self.transform(clean_waveform)\n",
        "            noisy_waveform = self.transform(noisy_waveform)\n",
        "\n",
        "        return clean_waveform, noisy_waveform  # Tuple of clean and noisy waveforms\n",
        "\n",
        "'''\n",
        "DataLoader Usage:\n",
        "# Create dataset instances for each split\n",
        "train_dataset = AudioDataset(base_dir=\"EARS-WHAM-16.0kHz\", dataset=\"train\", seg_length=16000)\n",
        "valid_dataset = AudioDataset(base_dir=\"EARS-WHAM-16.0kHz\", dataset=\"valid\", seg_length=16000)\n",
        "test_dataset = AudioDataset(base_dir=\"EARS-WHAM-16.0kHz\", dataset=\"test\", seg_length=16000)\n",
        "\n",
        "# Define DataLoader instances\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Example usage: Iterate through the training DataLoader\n",
        "for batch_idx, (clean_batch, noisy_batch) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}\")\n",
        "    print(f\"Clean Shape: {clean_batch.shape}, Noisy Shape: {noisy_batch.shape}\")\n",
        "\n",
        "For Training:\n",
        "for epoch in range(num_epochs):\n",
        "    for clean_batch, noisy_batch in train_loader:\n",
        "        # Move to GPU if available\n",
        "        clean_batch = clean_batch.to(device)\n",
        "        noisy_batch = noisy_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(noisy_batch)\n",
        "\n",
        "        # Loss calculation and optimization\n",
        "        loss = loss_function(output, clean_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} completed.\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "Z-dhbPGyPFQ1",
        "outputId": "754591c4-d1d5-4bd0-983e-ca9c0e413fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDataLoader Usage:\\n# Create dataset instances for each split\\ntrain_dataset = AudioDataset(base_dir=\"EARS-WHAM-16.0kHz\", dataset=\"train\", seg_length=16000)\\nvalid_dataset = AudioDataset(base_dir=\"EARS-WHAM-16.0kHz\", dataset=\"valid\", seg_length=16000)\\ntest_dataset = AudioDataset(base_dir=\"EARS-WHAM-16.0kHz\", dataset=\"test\", seg_length=16000)\\n\\n# Define DataLoader instances\\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\\n\\n# Example usage: Iterate through the training DataLoader\\nfor batch_idx, (clean_batch, noisy_batch) in enumerate(train_loader):\\n    print(f\"Batch {batch_idx + 1}\")\\n    print(f\"Clean Shape: {clean_batch.shape}, Noisy Shape: {noisy_batch.shape}\")\\n\\nFor Training:\\nfor epoch in range(num_epochs):\\n    for clean_batch, noisy_batch in train_loader:\\n        # Move to GPU if available\\n        clean_batch = clean_batch.to(device)\\n        noisy_batch = noisy_batch.to(device)\\n\\n        # Forward pass\\n        output = model(noisy_batch)\\n\\n        # Loss calculation and optimization\\n        loss = loss_function(output, clean_batch)\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n    print(f\"Epoch {epoch + 1} completed.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloader\n",
        "from data.dataset import EARSWHAMAudioDataset # custom class earlier\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class EARSWHAMDataLoader:\n",
        "    def __init__(self, base_dir=\"../datasets_final/EARS-WHAM16kHz\", seg_length=16000, batch_size=8, num_workers=4, transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.seg_length = seg_length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.transform = transform\n",
        "\n",
        "    def get_loader(self, split, shuffle=True):\n",
        "        dataset = EARSWHAMAudioDataset(base_dir=self.base_dir, dataset=split, transform=self.transform, seg_length=self.seg_length)\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=(shuffle if split == \"train\" else False),\n",
        "                            num_workers=self.num_workers, pin_memory=True)\n",
        "        return loader\n",
        "\n",
        "'''\n",
        "# Usage\n",
        "data_loader = EARSWHAMDataLoader(batch_size=8, seg_length=16000, num_workers=4)\n",
        "train_loader = data_loader.get_loader(\"train\")\n",
        "valid_loader = data_loader.get_loader(\"valid\", shuffle=False)\n",
        "test_loader = data_loader.get_loader(\"test\", shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "6iwscszNPkfg",
        "outputId": "64550baf-4970-405a-9dc7-d7ede8b8ec43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Usage\\ndata_loader = EARSWHAMDataLoader(batch_size=8, seg_length=16000, num_workers=4)\\ntrain_loader = data_loader.get_loader(\"train\")\\nvalid_loader = data_loader.get_loader(\"valid\", shuffle=False)\\ntest_loader = data_loader.get_loader(\"test\", shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This file implements both the non-causal and causal versions of Conv-TasNet.\n",
        "\n",
        "The implementation is based on the paper:\n",
        "\"Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation\"\n",
        "by Yi Luo and Nima Mesgarani.\n",
        "\n",
        "The code includes building blocks of Conv-TasNet and a wrapper to instantiate\n",
        "either the causal or non-causal version based on user preference.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CausalConv1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, groups=1):\n",
        "        super(CausalConv1D, self).__init__()\n",
        "        self.left_padding = (kernel_size - 1) * dilation\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            dilation=dilation,\n",
        "            groups=groups,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, (self.left_padding, 0))\n",
        "        return self.conv(x)\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    \"\"\"Non-Causal 1D Convolutional block.\n",
        "\n",
        "    This block uses symmetric padding, making it non-causal.\n",
        "\n",
        "    Args:\n",
        "        io_channels (int): Number of input/output channels, <B, Sc>.\n",
        "        hidden_channels (int): Number of channels in the internal layers, <H>.\n",
        "        kernel_size (int): Convolution kernel size of the middle layer, <P>.\n",
        "        padding (int): Symmetric padding value for convolution.\n",
        "        dilation (int, optional): Dilation value for convolution.\n",
        "        no_residual (bool, optional): Disable residual output.\n",
        "    \"\"\"\n",
        "    def __init__(self, io_channels: int, hidden_channels: int, kernel_size: int,\n",
        "                 dilation: int = 1, no_residual: bool = False):\n",
        "        super().__init__()\n",
        "        self.no_residual = no_residual\n",
        "        self.conv_layers = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(io_channels, hidden_channels, kernel_size=1),\n",
        "            torch.nn.PReLU(),\n",
        "            torch.nn.GroupNorm(1, hidden_channels, eps=1e-8),\n",
        "            torch.nn.Conv1d(\n",
        "                hidden_channels,\n",
        "                hidden_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                padding= (dilation * (kernel_size - 1) // 2),\n",
        "                dilation=dilation,\n",
        "                groups=hidden_channels,\n",
        "            ),\n",
        "            torch.nn.PReLU(),\n",
        "            torch.nn.GroupNorm(1, hidden_channels, eps=1e-8),\n",
        "        )\n",
        "        self.res_out = (\n",
        "            None if no_residual else torch.nn.Conv1d(hidden_channels, io_channels, kernel_size=1)\n",
        "        )\n",
        "        self.skip_out = torch.nn.Conv1d(hidden_channels, io_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
        "        feature = self.conv_layers(input)\n",
        "        residual = self.res_out(feature) if not self.no_residual else None\n",
        "        skip_out = self.skip_out(feature)\n",
        "        return residual, skip_out\n",
        "\n",
        "\n",
        "class CausalConvBlock(torch.nn.Module):\n",
        "    \"\"\"Causal 1D Convolutional block.\n",
        "\n",
        "    This block uses left-padding to ensure that future frames are not accessed.\n",
        "\n",
        "    Args:\n",
        "        io_channels (int): Number of input/output channels, <B, Sc>.\n",
        "        hidden_channels (int): Number of channels in the internal layers, <H>.\n",
        "        kernel_size (int): Convolution kernel size of the middle layer, <P>.\n",
        "        dilation (int, optional): Dilation value for convolution.\n",
        "        no_residual (bool, optional): Disable residual output.\n",
        "    \"\"\"\n",
        "    def __init__(self, io_channels: int, hidden_channels: int, kernel_size: int,\n",
        "                 dilation: int = 1, no_residual: bool = False):\n",
        "        super().__init__()\n",
        "        self.left_padding = (kernel_size - 1) * dilation\n",
        "        self.conv_layers = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(io_channels, hidden_channels, kernel_size=1),\n",
        "            torch.nn.PReLU(),\n",
        "            CausalConv1D(\n",
        "                hidden_channels,\n",
        "                hidden_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                dilation=dilation,\n",
        "                groups=hidden_channels,\n",
        "            ),\n",
        "            torch.nn.PReLU(),\n",
        "        )\n",
        "        self.res_out = (\n",
        "            None if no_residual else torch.nn.Conv1d(hidden_channels, io_channels, kernel_size=1)\n",
        "        )\n",
        "        self.skip_out = torch.nn.Conv1d(hidden_channels, io_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
        "        feature = self.conv_layers(input)\n",
        "        residual = self.res_out(feature) if self.res_out else None\n",
        "        skip_out = self.skip_out(feature)\n",
        "        return residual, skip_out\n",
        "\n",
        "\n",
        "class MaskGenerator(torch.nn.Module):\n",
        "    \"\"\"TCN-based Mask Generator for both causal and non-causal versions.\n",
        "\n",
        "    Args:\n",
        "        causal (bool): Use causal convolutions if True, otherwise non-causal.\n",
        "        Other arguments match the original Conv-TasNet implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_sources: int, kernel_size: int,\n",
        "                 num_feats: int, num_hidden: int, num_layers: int, num_stacks: int,\n",
        "                 msk_activate: str, causal: bool = False):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_sources = num_sources\n",
        "        self.input_norm = torch.nn.GroupNorm(1, input_dim, eps=1e-8)\n",
        "        self.input_conv = torch.nn.Conv1d(input_dim, num_feats, kernel_size=1)\n",
        "\n",
        "        self.receptive_field = 0\n",
        "        self.conv_layers = torch.nn.ModuleList([])\n",
        "        ConvBlockType = CausalConvBlock if causal else ConvBlock\n",
        "        for s in range(num_stacks):\n",
        "            for l in range(num_layers):\n",
        "                dilation = 2**l\n",
        "                self.conv_layers.append(\n",
        "                    ConvBlockType(\n",
        "                        io_channels=num_feats,\n",
        "                        hidden_channels=num_hidden,\n",
        "                        kernel_size=kernel_size,\n",
        "                        dilation=dilation,\n",
        "                        no_residual=(l == num_layers - 1 and s == num_stacks - 1),\n",
        "                    )\n",
        "                )\n",
        "                self.receptive_field += kernel_size if s == 0 and l == 0 else (kernel_size - 1) * dilation\n",
        "        self.output_prelu = torch.nn.PReLU()\n",
        "        self.output_conv = torch.nn.Conv1d(num_feats, input_dim * num_sources, kernel_size=1)\n",
        "        self.mask_activate = torch.nn.Sigmoid() if msk_activate == \"sigmoid\" else torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = input.shape[0]\n",
        "        feats = self.input_norm(input)\n",
        "        feats = self.input_conv(feats)\n",
        "        output = 0.0\n",
        "        for layer in self.conv_layers:\n",
        "            residual, skip = layer(feats)\n",
        "            if residual is not None:\n",
        "                feats = feats + residual\n",
        "            output = output + skip\n",
        "        output = self.output_prelu(output)\n",
        "        output = self.output_conv(output)\n",
        "        output = self.mask_activate(output)\n",
        "        return output.view(batch_size, self.num_sources, self.input_dim, -1)\n",
        "\n",
        "\n",
        "class ConvTasNet(torch.nn.Module):\n",
        "    \"\"\"Conv-TasNet implementation for both causal and non-causal settings.\n",
        "\n",
        "    Args:\n",
        "        causal (bool): Use causal convolutions if True, otherwise non-causal.\n",
        "        Other arguments match the original Conv-TasNet implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_sources: int = 2, enc_kernel_size: int = 16,\n",
        "                 enc_num_feats: int = 512, msk_kernel_size: int = 3,\n",
        "                 msk_num_feats: int = 128, msk_num_hidden_feats: int = 512,\n",
        "                 msk_num_layers: int = 8, msk_num_stacks: int = 3,\n",
        "                 msk_activate: str = \"sigmoid\", causal: bool = False):\n",
        "        super().__init__()\n",
        "        self.encoder = torch.nn.Conv1d(\n",
        "            1, enc_num_feats, kernel_size=enc_kernel_size,\n",
        "            stride=enc_kernel_size // 2, padding=enc_kernel_size // 2, bias=False,\n",
        "        )\n",
        "        self.mask_generator = MaskGenerator(\n",
        "            input_dim=enc_num_feats,\n",
        "            num_sources=num_sources,\n",
        "            kernel_size=msk_kernel_size,\n",
        "            num_feats=msk_num_feats,\n",
        "            num_hidden=msk_num_hidden_feats,\n",
        "            num_layers=msk_num_layers,\n",
        "            num_stacks=msk_num_stacks,\n",
        "            msk_activate=msk_activate,\n",
        "            causal=causal,\n",
        "        )\n",
        "        self.decoder = torch.nn.ConvTranspose1d(\n",
        "            enc_num_feats, 1, kernel_size=enc_kernel_size,\n",
        "            stride=enc_kernel_size // 2, padding=enc_kernel_size // 2, bias=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        padded, num_pads = self._align_num_frames_with_strides(input)\n",
        "        feats = self.encoder(padded)\n",
        "        masked = self.mask_generator(feats) * feats.unsqueeze(1)\n",
        "        output = self.decoder(masked.view(-1, feats.size(1), feats.size(2)))\n",
        "        return output.view(input.size(0), -1, input.size(2))\n",
        "\n",
        "    def _align_num_frames_with_strides(self, input: torch.Tensor) -> Tuple[torch.Tensor, int]:\n",
        "        num_frames = input.size(-1)\n",
        "        stride = self.encoder.stride[0]\n",
        "        pad = (stride - (num_frames % stride)) % stride\n",
        "        return torch.nn.functional.pad(input, (0, pad)), pad\n",
        "\n",
        "\n",
        "def build_conv_tasnet(causal: bool = False, **kwargs) -> ConvTasNet:\n",
        "    \"\"\"Wrapper to instantiate either causal or non-causal Conv-TasNet.\"\"\"\n",
        "    return ConvTasNet(causal=causal, **kwargs)\n",
        "\n",
        "'''\n",
        "Usage\n",
        "Instantiate Non-Causal Model:\n",
        "model = build_conv_tasnet(causal=False, num_sources=2)\n",
        "\n",
        "Instantiate Causal Model:\n",
        "model = build_conv_tasnet(causal=True, num_sources=2)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "qZuOQQN8PrJq",
        "outputId": "a83301e4-43e4-45b3-a232-c7017eea8873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUsage\\nInstantiate Non-Causal Model:\\nmodel = build_conv_tasnet(causal=False, num_sources=2)\\n\\nInstantiate Causal Model:\\nmodel = build_conv_tasnet(causal=True, num_sources=2)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "def calculate_fmaps(weight: torch.Tensor) -> tuple[int, int, int]:\n",
        "    num_outputs, num_inputs, *receptive_field_fmaps = weight.shape\n",
        "    receptive_field = math.prod(receptive_field_fmaps)\n",
        "    return num_inputs, num_outputs, receptive_field\n",
        "\n",
        "def calculate_fan_in_out(weight: torch.Tensor) -> tuple[int, int]:\n",
        "    fan_in, fan_out, receptive_fields = calculate_fmaps(weight)\n",
        "    return fan_in * receptive_fields, fan_out * receptive_fields\n",
        "\n",
        "def spectral_fan(fan_in: int, fan_out: int) -> float:\n",
        "    \"\"\"Parametrization 1 from https://arxiv.org/abs/2310.17813.\"\"\"\n",
        "    return fan_in**2 / min(fan_in, fan_out)\n",
        "\n",
        "@torch.no_grad()\n",
        "def spectral_normal_(weight: torch.Tensor, gain: float = 1.0) -> None:\n",
        "    fan_in, fan_out = calculate_fan_in_out(weight)\n",
        "    fan = spectral_fan(fan_in, fan_out)\n",
        "    std = gain / math.sqrt(fan)\n",
        "    weight.normal_(0, std)\n",
        "\n",
        "def apply_spectral_initialization(model: torch.nn.Module, gain: float = 1.0) -> None:\n",
        "    \"\"\"Applies spectral initialization to all Conv1D layers in the model.\"\"\"\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, torch.nn.Conv1d) or isinstance(module, torch.nn.ConvTranspose1d):\n",
        "            spectral_normal_(module.weight, gain=gain)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n"
      ],
      "metadata": {
        "id": "GQNKXKUSQmsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}